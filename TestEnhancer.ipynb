{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements"
      ],
      "metadata": {
        "id": "BIZNV2FpqyiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c4VlvYTqkqR"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestEnhancer"
      ],
      "metadata": {
        "id": "nCg-gLJ4q2D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "define_system_prompt = \"\"\"\n",
        "You are an expert in generating runnable Python test cases using `pytest`.\n",
        "\n",
        "TASK:\n",
        "Your task is to generate exactly **two** test cases per run:\n",
        "- **One pass-to-pass (ptp) test** for valid inputs.\n",
        "- **One fail-to-pass (ftp) test** for invalid inputs and edge cases.\n",
        "\n",
        "EXPECTED OUTPUT:\n",
        "1. Each test case must:\n",
        "   - Be a separate function with a meaningful name prefixed by:\n",
        "      - `test_ptp_` for pass-to-pass tests.\n",
        "      - `test_ftp_` for fail-to-pass tests.\n",
        "   - Contain **one or two assertions** at most.\n",
        "   - Provide setup code and context before assertions.\n",
        "2. Use proper `pytest` syntax, such as `pytest.raises` for exceptions.\n",
        "3. Ensure the test is self-contained and runnable with `pytest`.\n",
        "\n",
        "Buggy Code:\n",
        "{buggy_code}\n",
        "\n",
        "Gold Patch:\n",
        "{gold_patch}\n",
        "\n",
        "Original Tests:\n",
        "{tests}\n",
        "\n",
        "### OUTPUT FORMAT:\n",
        "Provide exactly **two** test cases in a Python code block. The test functions should:\n",
        "1. Begin with `test_ptp_` or `test_ftp_` depending on the category.\n",
        "2. Include setup code and one or two assertions.\n",
        "3. only python code (no comments or descriptions)\n",
        "\"\"\"\n",
        "\n",
        "human_prompt = \"\"\"\n",
        "Analyze the following code changes and generate exactly **two** test cases:\n",
        "- **One pass-to-pass test** for valid inputs.\n",
        "- **One fail-to-pass test** for invalid inputs and edge cases.\n",
        "\n",
        "Buggy Code:\n",
        "{buggy_code}\n",
        "\n",
        "Gold Patch:\n",
        "{gold_patch}\n",
        "\n",
        "Original Tests:\n",
        "{tests}\n",
        "\n",
        "Ensure the two test cases follow the required format and are runnable with pytest and only python code (no comments or descriptions)\n",
        "\n",
        "\"\"\"\n",
        "# Combine into a ChatPromptTemplate\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(define_system_prompt),\n",
        "        HumanMessagePromptTemplate.from_template(human_prompt),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Set API key securely\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\n",
        "\n",
        "# Initialize the LLM with GPT-4\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "# Create the pipeline\n",
        "test_case_pipeline = template | llm\n",
        "\n",
        "\n",
        "def process_project_instances(file_path, project_name, output_file, skipped_instances_file):\n",
        "    \"\"\"\n",
        "    Processes instances related to a specific project and writes all test cases into a single file.\n",
        "\n",
        "    Each instance runs 20 times, generating one test per run.\n",
        "    If an instance exceeds the token limit, it is skipped and logged.\n",
        "    \"\"\"\n",
        "    # Load the Excel file\n",
        "    data = pd.read_excel(file_path)\n",
        "\n",
        "    # Filter data for the specified project\n",
        "    project_data = data[data['repo'] == project_name]\n",
        "\n",
        "    # Open the output file in write mode\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as consolidated_file, \\\n",
        "         open(skipped_instances_file, \"w\", encoding=\"utf-8\") as skipped_file:\n",
        "\n",
        "        consolidated_file.write(\"import pytest\\n\\n\")  # Ensure pytest is imported\n",
        "\n",
        "        for idx, row in project_data.iterrows():\n",
        "            instance_id = row[\"instance_id\"]\n",
        "            buggy_code = row[\"buggy_patch\"]\n",
        "            gold_patch = row[\"patch\"]\n",
        "            tests = row[\"test_patch\"]\n",
        "\n",
        "            if pd.isna(buggy_code) or pd.isna(gold_patch) or pd.isna(tests):\n",
        "                print(f\"Skipping instance {instance_id} due to missing data.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing instance {instance_id} for project {project_name}...\")\n",
        "\n",
        "            inputs = {\n",
        "                \"buggy_code\": buggy_code,\n",
        "                \"gold_patch\": gold_patch,\n",
        "                \"tests\": tests,\n",
        "            }\n",
        "\n",
        "            # Write a section header for this instance\n",
        "            consolidated_file.write(f\"\\n### Instance: {instance_id} (Project: {project_name})\\n\\n\")\n",
        "\n",
        "            try:\n",
        "                for run in range(1, 21):  # Running the prompt 20 times per instance\n",
        "                    response = test_case_pipeline.invoke(inputs)\n",
        "                    output = response.content if hasattr(response, \"content\") else \"No content\"\n",
        "\n",
        "                    # Check for token limit errors\n",
        "                    if \"maximum context length\" in output.lower() or \"token limit\" in output.lower():\n",
        "                        print(f\"Skipping instance {instance_id}: Token limit exceeded.\")\n",
        "                        skipped_file.write(f\"{instance_id}\\n\")\n",
        "                        break  # Skip further runs for this instance\n",
        "\n",
        "                    # Ensure proper output is generated\n",
        "                    if \"```python\" not in output and \"```\" not in output:\n",
        "                        print(f\"Skipping instance {instance_id} run {run}: No valid test cases generated.\")\n",
        "                        continue\n",
        "\n",
        "                    # Clean the output and remove unnecessary formatting\n",
        "                    output_lines = output.splitlines()\n",
        "                    cleaned_output = \"\\n\".join(\n",
        "                        line for line in output_lines if line.strip() not in [\"```\", \"```python\"]\n",
        "                    )\n",
        "\n",
        "                    # Write a run-specific header\n",
        "                    consolidated_file.write(f\"# --- Run {run} ---\\n\\n\")\n",
        "                    consolidated_file.write(cleaned_output + \"\\n\\n\")\n",
        "\n",
        "                    print(f\"Generated test {run} for instance {instance_id}.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing instance {instance_id}: {e}\")\n",
        "                skipped_file.write(f\"{instance_id}\\n\")  # Log the skipped instance\n",
        "\n",
        "    print(f\"\\nAll test cases for {project_name} have been written to {output_file}.\")\n",
        "    print(f\"Skipped instances have been logged in {skipped_instances_file}.\")\n",
        "\n",
        "\n",
        "# Run the script for a specific project\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"astropy_temp.xlsx\"\n",
        "    selected_project = \"astropy/astropy\"  # Change this to the desired project\n",
        "    output_file = f\"all_tests_{selected_project.replace('/', '__')}.py\"\n",
        "    skipped_instances_file = \"requests_skipped_instances.txt\"\n",
        "\n",
        "    process_project_instances(file_path, selected_project, output_file, skipped_instances_file)\n",
        "    print(f\"\\nFinal test cases have been written to {output_file}.\")\n"
      ],
      "metadata": {
        "id": "-GsGjDPCqqYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}