{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements"
      ],
      "metadata": {
        "id": "l3t_VQFIsm-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "-RGTeBA2sh4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data"
      ],
      "metadata": {
        "id": "xMmmcZUOstrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_file = 'swe.parquet' #full swe-bench data from huggingface\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_parquet(input_file)\n",
        "print(len(data))\n",
        "\n",
        "# Filter the dataset\n",
        "filtered_data = data\n",
        "# Save the filtered dataset\n",
        "filtered_data.to_parquet(output_file)\n",
        "\n",
        "print(f\"Filtered dataset saved to {output_file}.\")\n"
      ],
      "metadata": {
        "id": "f2luaQ1OsRa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SoluLeakDetector"
      ],
      "metadata": {
        "id": "0caOleOctEpn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg8_zMwSsKmf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Define the system prompt\n",
        "define_system_prompt = \"\"\"\n",
        "You are a solution leakage detection expert.\n",
        "\n",
        "TASK:\n",
        "Your task is to analyze GitHub issue descriptions (`problem_statement`) and related comments (`hints_text`) for solution leakage.\n",
        "\n",
        "DEFINITION:\n",
        "Solution leakage occurs when:\n",
        "1. The solution is explicitly mentioned (e.g., code snippets or direct instructions).\n",
        "2. The solution is subtly implied (e.g., explanatory text or hints that lead directly to a solution).\n",
        "\n",
        "EXAMPLES:\n",
        "Example 1:\n",
        "Description:\n",
        "I propose to add the following settings, with the following default values:\n",
        "\n",
        "LANGUAGE_COOKIE_SECURE = False\n",
        "LANGUAGE_COOKIE_HTTPONLY = False\n",
        "LANGUAGE_COOKIE_SAMESITE = None\n",
        "The default values maintain the current behavior.\n",
        "\n",
        "These settings do not provide much security value, since the language is not secret or sensitive. This was also discussed briefly here: ​https://github.com/django/django/pull/8380#discussion_r112448195. The reasons I'd like to add them are:\n",
        "\n",
        "Sometimes auditors require them.\n",
        "I personally prefer to set them unless I have a reason *not* to.\n",
        "Browsers are starting to strongly nudge toward HttpOnly and Secure when possible, e.g. ​https://webkit.org/blog/8613/intelligent-tracking-prevention-2-1/.\n",
        "\n",
        "Expected Output:\n",
        "{{\n",
        "  \"solution_leakage_detected\": true,\n",
        "  \"reason\": \"The solution is explicitly provided in the description.\",\n",
        "  \"extracted_solution\": \"LANGUAGE_COOKIE_SECURE = False, LANGUAGE_COOKIE_HTTPONLY = False, LANGUAGE_COOKIE_SAMESITE = None\"\n",
        "}}\n",
        "\n",
        "Example 2:\n",
        "Description:\n",
        "Shape of coef_ wrong for linear_model.Lasso when using fit_intercept=False\n",
        "\n",
        "Steps/Code to Reproduce\n",
        "Example:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "\n",
        "est_intercept = linear_model.Lasso(fit_intercept=True)\n",
        "est_intercept.fit(np.c_[np.ones(3)], np.ones(3))\n",
        "assert est_intercept.coef_.shape  == (1,)\n",
        "\n",
        "est_no_intercept = linear_model.Lasso(fit_intercept=False)\n",
        "est_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))\n",
        "assert est_no_intercept.coef_.shape  == (1,)\n",
        "\n",
        "Expected Output:\n",
        "{{\n",
        "  \"solution_leakage_detected\": false,\n",
        "  \"reason\": \"The description identifies a bug but does not explicitly provide a solution.\",\n",
        "  \"extracted_solution\": null\n",
        "}}\n",
        "\n",
        "Example 3:\n",
        "Description:\n",
        "There is a typo in Poly3DCollection.__init__() that causes a TypeError exception whenever the function is called with shade=True.\n",
        "\n",
        "matplotlib/lib/mpl_toolkits/mplot3d/art3d.py\n",
        "\n",
        "Line 908 in f7a8cab\n",
        "\n",
        " if facecolors is None and edgecolors in None:\n",
        "edgecolors in None should be edgecolors is None\n",
        "\n",
        "Expected Output:\n",
        "{{\n",
        "  \"solution_leakage_detected\": true,\n",
        "  \"reason\": \"The solution is explicitly provided as a corrected code snippet.\",\n",
        "  \"extracted_solution\": \"edgecolors in None should be edgecolors is None\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Define the template for the prompt\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(define_system_prompt),\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"Analyze the following problem and comments for solution leakage.\\n\\nProblem Statement:\\n{problem_statement}\\n\\nHints Text:\\n{hints_text}\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize the LLM with GPT-4\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0, openai_api_key=key1)\n",
        "\n",
        "# Create the pipeline\n",
        "leakage_detection_pipeline = template | llm\n",
        "\n",
        "# Function to run solution leakage detection\n",
        "def detect_solution_leakage(problem_statement, hints_text):\n",
        "    inputs = {\n",
        "        \"problem_statement\": problem_statement,\n",
        "        \"hints_text\": hints_text\n",
        "    }\n",
        "    response = leakage_detection_pipeline.invoke(inputs)\n",
        "    # Handle response content as plain text\n",
        "    if hasattr(response, \"content\"):\n",
        "        return parse_response_content(response.content)\n",
        "    return {}\n",
        "\n",
        "# Function to parse the response content\n",
        "def parse_response_content(content):\n",
        "    \"\"\"\n",
        "    Parses the response content, which is plain text, into a dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt JSON parsing if applicable\n",
        "        parsed_response = json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fall back to simple plain text parsing\n",
        "        lines = content.split(\"\\n\")\n",
        "        parsed_response = {\"raw_text\": content}\n",
        "        for line in lines:\n",
        "            if \"solution_leakage_detected\" in line.lower():\n",
        "                parsed_response[\"solution_leakage_detected\"] = \"true\" in line.lower()\n",
        "            elif \"reason\" in line.lower():\n",
        "                parsed_response[\"reason\"] = line.split(\":\", 1)[-1].strip()\n",
        "            elif \"extracted_solution\" in line.lower():\n",
        "                parsed_response[\"extracted_solution\"] = line.split(\":\", 1)[-1].strip()\n",
        "    return parsed_response\n",
        "\n",
        "# Function to format results\n",
        "def format_result(instance_id, problem_index, problem_statement, result, is_leakage_type):\n",
        "    return {\n",
        "        \"Instance ID\": instance_id,\n",
        "        \"Problem Index\": problem_index,\n",
        "        \"Leakage Type\": is_leakage_type,\n",
        "        \"Problem Statement\": problem_statement,\n",
        "        \"Reason\": result.get(\"reason\", \"No reason provided\"),\n",
        "        \"Extracted Solution\": result.get(\"extracted_solution\", \"No solution extracted\")\n",
        "    }\n",
        "\n",
        "# Function to process a dataset and analyze rows\n",
        "def detect_and_organize_results_with_links(file_path, output_file):\n",
        "    data = pd.read_parquet(file_path)\n",
        "\n",
        "    if 'instance_id' not in data.columns or 'problem_statement' not in data.columns or 'hints_text' not in data.columns:\n",
        "        raise ValueError(\"The dataset must contain 'instance_id', 'problem_statement', and 'hints_text' columns.\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        if len(row['problem_statement']) + len(row['hints_text']) > 7000:  # Filter long instances\n",
        "            print(f\"Skipping instance {row['instance_id']} due to excessive length.\")\n",
        "            continue\n",
        "        instance_id = row['instance_id']\n",
        "        problem_statement = row['problem_statement']\n",
        "        hints_text = row['hints_text']\n",
        "        raw_result = detect_solution_leakage(problem_statement, hints_text)\n",
        "\n",
        "        # Determine leakage type\n",
        "        if raw_result.get(\"solution_leakage_detected\", False):\n",
        "            if any(kw in raw_result.get(\"reason\", \"\").lower() for kw in [\"explicitly mentioned\", \"explicitly provided\", \"clear solution\", \"direct instructions\", \"patch file\", \"code snippet\"]):\n",
        "                is_leakage_type = \"Solution Leak - Direct\"\n",
        "            elif any(kw in raw_result.get(\"reason\", \"\").lower() for kw in [\"hint\", \"subtly implied\", \"suggests\"]):\n",
        "                is_leakage_type = \"Solution Leak - Hint\"\n",
        "            else:\n",
        "                is_leakage_type = \"Solution Leak - Direct\"\n",
        "        else:\n",
        "            is_leakage_type = \"No Solution Leak\"\n",
        "\n",
        "        # Organize the output\n",
        "        formatted_result = format_result(instance_id, idx + 1, problem_statement, raw_result, is_leakage_type)\n",
        "        results.append(formatted_result)\n",
        "\n",
        "    # Save results to output file\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "        # Calculate statistics\n",
        "    stats = {\n",
        "        \"Solution Leak - Direct\": sum(1 for r in results if r[\"Leakage Type\"] == \"Solution Leak - Direct\"),\n",
        "        \"Solution Leak - Hint\": sum(1 for r in results if r[\"Leakage Type\"] == \"Solution Leak - Hint\"),\n",
        "        \"No Solution Leak\": sum(1 for r in results if r[\"Leakage Type\"] == \"No Solution Leak\")\n",
        "    }\n",
        "\n",
        "    # Add statistics to the output file\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump({\"results\": results, \"statistics\": stats}, f, indent=4)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = 'filtered_swe_bench_data.parquet'\n",
        "    output_file = 'results.json'\n",
        "\n",
        "    try:\n",
        "        organized_results = detect_and_organize_results_with_links(file_path, output_file)\n",
        "        print(f\"Results saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n"
      ]
    }
  ]
}